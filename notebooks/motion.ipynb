{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "from justpfm import justpfm\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.optimize import curve_fit\n",
    "from skimage.measure import shannon_entropy\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "tqdm.pandas()\n",
    "\n",
    "from src.utils.gaze_data import get_gaze_data\n",
    "from src.utils.file import get_files_recursive, get_ids_from_file_path, get_set_str\n",
    "from src.config import (\n",
    "    SALIENCY_MAP_PFM_PATH, \n",
    "    RAW_GAZE_FRAME_WIDTH,\n",
    "    RAW_GAZE_FRAME_HEIGHT,\n",
    "    IMAGE_WIDTH,\n",
    "    IMAGE_HEIGHT,\n",
    "    DEPTH_MAP_PFM_PATH,\n",
    "    DEPTH_SEG_PFM_PATH,\n",
    "    SETS_PATH,\n",
    ")\n",
    "\n",
    "FRAME_STEP = 5\n",
    "\n",
    "N_NSEC_IN_SEC = 1e9\n",
    "FPS = 25\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Fixation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixation_data(\n",
    "    frame_step: int = FRAME_STEP,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get fixation data for videos.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Fixation data for videos\n",
    "    \"\"\"\n",
    "    fixation_data_1 = get_gaze_data(\n",
    "        experiment_ids=[1],\n",
    "        set_ids=[0],\n",
    "        fixation=True,\n",
    "    )\n",
    "    fixation_data_2 = get_gaze_data(\n",
    "        experiment_ids=[2],\n",
    "        fixation=True,\n",
    "    )\n",
    "    fixation_data = pd.concat([fixation_data_1, fixation_data_2])\n",
    "\n",
    "    # Sort data and add Fixation Id\n",
    "    fixation_data = fixation_data.sort_values(by=[\"SequenceId\", \"StartTimestamp_ns\"])\n",
    "    fixation_data[\"FixationId\"] = np.arange(len(fixation_data))\n",
    "\n",
    "    # Scale fixation coordinates\n",
    "    fixation_data[\"X_px\"] = fixation_data[\"X_px\"] * IMAGE_WIDTH / RAW_GAZE_FRAME_WIDTH\n",
    "    fixation_data[\"Y_px\"] = fixation_data[\"Y_px\"] * IMAGE_HEIGHT / RAW_GAZE_FRAME_HEIGHT\n",
    "\n",
    "    # Get Frame Id per step\n",
    "    fixation_data[\"FrameId\"] = (fixation_data[\"TimeSinceStart_ns\"] / N_NSEC_IN_SEC * FPS).astype(int)\n",
    "    fixation_data[\"FrameId\"] = (fixation_data[\"FrameId\"] / frame_step).astype(int) * frame_step\n",
    "\n",
    "    fixation_data = fixation_data[[\n",
    "        \"FixationId\",\n",
    "        \"ExperimentId\",\n",
    "        \"SetId\",\n",
    "        \"SequenceId\",\n",
    "        \"FrameId\",\n",
    "        \"X_px\",\n",
    "        \"Y_px\",\n",
    "    ]]\n",
    "\n",
    "    return fixation_data\n",
    "\n",
    "fixation_data = get_fixation_data()\n",
    "fixation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Null Fixation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_saliency_map() -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get the global saliency map by merging all saliency maps\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: global saliency map\n",
    "    \"\"\"\n",
    "    saliency_file_paths = Path(SALIENCY_MAP_PFM_PATH).rglob(\"*.pfm\")\n",
    "    saliency_file_paths = sorted(saliency_file_paths)\n",
    "\n",
    "    global_saliency_map = None\n",
    "    for saliency_file_path in tqdm(\n",
    "        saliency_file_paths, desc=\"⌛ Loading saliency maps...\"\n",
    "    ):\n",
    "        # Load map and merge\n",
    "        saliency_map = justpfm.read_pfm(saliency_file_path)\n",
    "        if global_saliency_map is None:\n",
    "            global_saliency_map = saliency_map\n",
    "        else:\n",
    "            global_saliency_map += saliency_map\n",
    "\n",
    "    # To unit distribution\n",
    "    global_saliency_map /= global_saliency_map.sum()\n",
    "    global_saliency_map = np.squeeze(global_saliency_map)\n",
    "\n",
    "    return global_saliency_map\n",
    "\n",
    "\n",
    "def gaussian_2d(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    x0: float,\n",
    "    y0: float,\n",
    "    sigma_x: float,\n",
    "    sigma_y: float,\n",
    "    A: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    A 2D Gaussian function\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): x values\n",
    "        y (np.ndarray): y values\n",
    "        x0 (float): x center\n",
    "        y0 (float): y center\n",
    "        sigma_x (float): x standard deviation\n",
    "        sigma_y (float): y standard deviation\n",
    "        A (float): amplitude\n",
    "    \"\"\"\n",
    "\n",
    "    return A * np.exp(\n",
    "        -((x - x0) ** 2 / (2 * sigma_x**2) + (y - y0) ** 2 / (2 * sigma_y**2))\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_gaussian(\n",
    "    data: np.ndarray,\n",
    "    width: int,\n",
    "    height: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Fit a 2D Gaussian to the given data\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): 2D data to fit\n",
    "        width (int): width of the data\n",
    "        height (int): height of the data\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: fitted Gaussian\n",
    "    \"\"\"\n",
    "    # Create a 2D grid\n",
    "    x, y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    # Flatten the arrays to fit with curve_fit\n",
    "    x_data = np.vstack((x.ravel(), y.ravel()))\n",
    "    z_data = data.ravel()\n",
    "\n",
    "    # Start with an initial guess for (x0, y0, sigma_x, sigma_y, A)\n",
    "    initial_guess = (width // 2, height // 2, 20, 30, 1)\n",
    "\n",
    "    # Fit the Gaussian model to the data\n",
    "    popt, _ = curve_fit(\n",
    "        lambda xdata, x0, y0, sigma_x, sigma_y, A: gaussian_2d(\n",
    "            xdata[0], xdata[1], x0, y0, sigma_x, sigma_y, A\n",
    "        ),\n",
    "        x_data,\n",
    "        z_data,\n",
    "        p0=initial_guess,\n",
    "    )\n",
    "\n",
    "    # Get the optimized parameters\n",
    "    x0, y0, sigma_x, sigma_y, A = popt\n",
    "    print(\n",
    "        f\"Optimized parameters: x0={x0}, y0={y0}, sigma_x={sigma_x}, sigma_y={sigma_y}, A={A}\"\n",
    "    )\n",
    "\n",
    "    # Generate the fitted Gaussian using the optimized parameters\n",
    "    fitted_gaussian = gaussian_2d(x, y, x0, y0, sigma_x, sigma_y, A)\n",
    "\n",
    "    return fitted_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_saliency_map = get_global_saliency_map()\n",
    "fitted_gaussian = fit_gaussian(\n",
    "    data=global_saliency_map,\n",
    "    width=global_saliency_map.shape[1],\n",
    "    height=global_saliency_map.shape[0],\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(global_saliency_map, cmap=\"jet\")\n",
    "plt.colorbar(label=\"Saliency map\")\n",
    "plt.title(\"Global saliency map\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(fitted_gaussian, cmap=\"jet\")\n",
    "plt.colorbar(label=\"Saliency map\")\n",
    "plt.title(\"Fitted gaussian distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_fixation_data(fixation_data: pd.DataFrame, fitted_gaussian: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get a null fixation data by sampling from a fitted Gaussian distribution\n",
    "\n",
    "    Args:\n",
    "        fixation_data (pd.Dataframe): The fixation data\n",
    "        fitted_gaussian (np.ndarray): The fitted Gaussian distribution\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The null fixation data\n",
    "    \"\"\"\n",
    "    # Get the x and y coordinates\n",
    "    x, y = np.meshgrid(np.arange(fitted_gaussian.shape[1]), np.arange(fitted_gaussian.shape[0]))\n",
    "\n",
    "    # Flatten the arrays\n",
    "    x = x.ravel()\n",
    "    y = y.ravel()\n",
    "    z = fitted_gaussian.ravel()\n",
    "\n",
    "    # Normalize the z values\n",
    "    z /= z.sum()\n",
    "\n",
    "    # Sample from the fitted Gaussian distribution\n",
    "    sampled_indices = np.random.choice(np.arange(len(x)), size=len(fixation_data), p=z)\n",
    "    sampled_x = x[sampled_indices]\n",
    "    sampled_y = y[sampled_indices]\n",
    "\n",
    "    # Create a new DataFrame\n",
    "    null_fixation_data = fixation_data.copy()\n",
    "    null_fixation_data[\"X_px\"] = sampled_x\n",
    "    null_fixation_data[\"Y_px\"] = sampled_y\n",
    "\n",
    "    return null_fixation_data\n",
    "\n",
    "null_fixation_data = get_null_fixation_data(fixation_data=fixation_data, fitted_gaussian=fitted_gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_null_fixation_data = null_fixation_data.sample(n=10_000, random_state=SEED)\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.imshow(fitted_gaussian, cmap=\"magma\")\n",
    "plt.colorbar(label=\"Saliency map\")\n",
    "plt.title(\"Fitted gaussian distribution\")\n",
    "plt.scatter(subsampled_null_fixation_data[\"X_px\"], subsampled_null_fixation_data[\"Y_px\"], color=\"green\", s=2)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_data(\n",
    "    folder_path: str, \n",
    "    file_extension: str,\n",
    ") -> Dict[int, Dict[int, Dict[int, np.ndarray]]]:\n",
    "    file_paths = get_files_recursive(folder_path, f\"*.{file_extension}\")\n",
    "    feature_data = {}\n",
    "    for file_path in file_paths:\n",
    "        # Get ids and create empty dict\n",
    "        experiment_id, set_id, sequence_id = get_ids_from_file_path(file_path)\n",
    "        if experiment_id not in feature_data:\n",
    "            feature_data[experiment_id] = {}\n",
    "        if set_id not in feature_data[experiment_id]:\n",
    "            feature_data[experiment_id][set_id] = {}\n",
    "        if sequence_id not in feature_data[experiment_id][set_id]:\n",
    "            feature_data[experiment_id][set_id][sequence_id] = {}\n",
    "\n",
    "        # Load data and add to dict\n",
    "        data = justpfm.read_pfm(file_path)\n",
    "        feature_data[experiment_id][set_id][sequence_id] = data\n",
    "\n",
    "    return feature_data\n",
    "\n",
    "\n",
    "def get_feature_value(\n",
    "    data: Dict[int, Dict[int, Dict[int, np.ndarray]]] | np.ndarray,\n",
    "    row: pd.Series,\n",
    "    transform: callable = None,\n",
    "    r: int = 5,\n",
    ") -> float:\n",
    "    # Retrieve image\n",
    "    if isinstance(data, Dict):\n",
    "        experiment_id = row[\"ExperimentId\"]\n",
    "        set_id = row[\"SetId\"]\n",
    "        sequence_id = row[\"SequenceId\"]\n",
    "        data = data[experiment_id][set_id][sequence_id]\n",
    "\n",
    "    # Get the shape and define the region of interest\n",
    "    x = row[\"X_px\"]\n",
    "    y = row[\"Y_px\"]\n",
    "    height, width = data.shape[:2]\n",
    "    x_min = int(max(0, x - r))\n",
    "    x_max = int(min(width, x + r + 1))\n",
    "    y_min = int(max(0, y - r))\n",
    "    y_max = int(min(height, y + r + 1))\n",
    "\n",
    "    # Apply transform if available\n",
    "    if transform is not None:\n",
    "        data = transform(data)\n",
    "\n",
    "    # Extract the region of interest and calculate the mean\n",
    "    roi = data[y_min:y_max, x_min:x_max]\n",
    "    value = roi.mean()\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_brightness(image: np.ndarray) -> float:\n",
    "    brightness = np.mean(image, axis=2)[..., np.newaxis]\n",
    "    return brightness\n",
    "\n",
    "\n",
    "def get_colorfulness(image: np.ndarray) -> np.ndarray:\n",
    "    rg = np.abs(image[:, :, 0] - image[:, :, 1])\n",
    "    yb = np.abs(0.5 * (image[:, :, 0] + image[:, :, 1]) - image[:, :, 2])\n",
    "    colorfulness_map = np.sqrt(rg ** 2 + yb ** 2)\n",
    "    return colorfulness_map\n",
    "\n",
    "\n",
    "def get_saturation(image: np.ndarray) -> np.ndarray:\n",
    "    hsv_image = mcolors.rgb_to_hsv(image / 255.0)\n",
    "    saturation_map = hsv_image[:, :, 1]\n",
    "    return saturation_map\n",
    "\n",
    "\n",
    "def get_hue(image: np.ndarray) -> np.ndarray:\n",
    "    hsv_image = mcolors.rgb_to_hsv(image / 255.0)\n",
    "    hue_map = hsv_image[:, :, 0]\n",
    "    return hue_map\n",
    "\n",
    "\n",
    "def get_gradient_magnitude(image: np.ndarray) -> np.ndarray:\n",
    "    grayscale_image = np.mean(image, axis=2).astype(np.uint8)\n",
    "    gradient_x = cv2.Sobel(grayscale_image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    gradient_y = cv2.Sobel(grayscale_image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    gradient_magnitude = np.sqrt(gradient_x**2 + gradient_y**2)\n",
    "    return gradient_magnitude\n",
    "\n",
    "\n",
    "def get_local_contrast(image: np.ndarray, patch_size: int = 10) -> np.ndarray:\n",
    "    grayscale_image = np.mean(image, axis=2)\n",
    "    kernel = np.ones((patch_size, patch_size)) / (patch_size ** 2)\n",
    "    local_mean = cv2.filter2D(grayscale_image, -1, kernel)\n",
    "    local_contrast = np.sqrt((grayscale_image - local_mean) ** 2)\n",
    "    return local_contrast\n",
    "\n",
    "\n",
    "def get_local_sharpness(image: np.ndarray, patch_size: int = 10) -> np.ndarray:\n",
    "    grayscale_image = np.mean(image, axis=2).astype(np.uint8)\n",
    "    laplacian = cv2.Laplacian(grayscale_image, cv2.CV_64F)\n",
    "    kernel = np.ones((patch_size, patch_size)) / (patch_size ** 2)\n",
    "    local_sharpness = cv2.filter2D(np.abs(laplacian), -1, kernel)\n",
    "    return local_sharpness\n",
    "\n",
    "def get_local_entropy(image: np.ndarray, patch_size: int = 10) -> np.ndarray:\n",
    "    grayscale_image = np.mean(image, axis=2).astype(np.uint8)\n",
    "    entropy_map = np.zeros(grayscale_image.shape)\n",
    "    for i in range(0, grayscale_image.shape[0], patch_size):\n",
    "        for j in range(0, grayscale_image.shape[1], patch_size):\n",
    "            patch = grayscale_image[i:i+patch_size, j:j+patch_size]\n",
    "            entropy_map[i:i+patch_size, j:j+patch_size] = shannon_entropy(patch)\n",
    "    return entropy_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⌛ Loading depth features...: 100%|██████████| 2/2 [00:00<00:00,  8.15it/s]\n",
      "100%|██████████| 337638/337638 [00:04<00:00, 69636.50it/s]\n",
      "100%|██████████| 337638/337638 [00:04<00:00, 70513.13it/s]\n",
      "Processing video 1:   0%|          | 0/585 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def process_frame(group, video):\n",
    "    # Get the frame data\n",
    "    frame_id = group[\"FrameId\"].values[0]\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        raise ValueError(f\"❌ Frame {frame_id} could not be read.\")\n",
    "\n",
    "    # Apply the feature extraction for all fixations in this frame\n",
    "    group[\"Brightness\"] = group.apply(\n",
    "        lambda x: get_feature_value(data=frame, row=x, transform=get_brightness), axis=1\n",
    "    )\n",
    "    \n",
    "    return group\n",
    "\n",
    "\n",
    "def process_video(group, sets_path):\n",
    "    # Get group ids and video\n",
    "    experiment_id = group[\"ExperimentId\"].values[0]\n",
    "    set_id = group[\"SetId\"].values[0]\n",
    "    sequence_id = group[\"SequenceId\"].values[0]\n",
    "    set_str = get_set_str(experiment_id=experiment_id, set_id=set_id)\n",
    "    video_path = f\"{sets_path}/experiment{experiment_id}/{set_str}/scene{sequence_id:02}.mp4\"\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Process frames\n",
    "    grouped_data = group.groupby(\"FrameId\")\n",
    "    frame_rows = [row for _, row in grouped_data]\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        with tqdm(total=len(frame_rows), desc=f\"Processing video {sequence_id}\") as bar:\n",
    "            futures = [executor.submit(process_frame, row, video) for row in frame_rows]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "                bar.update(1)\n",
    "\n",
    "    video.release()\n",
    "\n",
    "    return pd.concat(results)\n",
    "\n",
    "\n",
    "def get_fixation_features(data: pd.DataFrame, sets_path: str) -> pd.DataFrame:\n",
    "    # Get depth features\n",
    "    bar = tqdm(total=2, desc=\"⌛ Loading depth features...\")\n",
    "    depth_maps = get_feature_data(DEPTH_MAP_PFM_PATH, \"pfm\")\n",
    "    bar.update(1)\n",
    "    depth_segs = get_feature_data(DEPTH_SEG_PFM_PATH, \"pfm\")\n",
    "    bar.update(1)\n",
    "    bar.close()\n",
    "\n",
    "    data[\"Depth\"] = data.progress_apply(\n",
    "        lambda x: get_feature_value(data=depth_maps, row=x),\n",
    "        axis=1,\n",
    "    )\n",
    "    data[\"DepthSeg\"] = data.progress_apply(\n",
    "        lambda x: get_feature_value(data=depth_segs, row=x),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Group by experiment, set, and sequence\n",
    "    grouped_data = data.groupby([\"ExperimentId\", \"SetId\", \"SequenceId\"])\n",
    "    results = grouped_data.apply(lambda group: process_video(group, sets_path))\n",
    "    results = results.reset_index(drop=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "fixation_data = get_fixation_features(fixation_data, SETS_PATH)\n",
    "null_fixation_data = get_fixation_features(null_fixation_data, SETS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(group, video):\n",
    "    # Get the frame data\n",
    "    frame_id = group[\"FrameId\"].values[0]\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        raise ValueError(f\"❌ Frame {frame_id} could not be read.\")\n",
    "\n",
    "    # Apply the feature extraction for all fixations in this frame\n",
    "    group[\"Brightness\"] = group.apply(\n",
    "        lambda x: get_feature_value(data=frame, row=x, transform=get_brightness), axis=1\n",
    "    )\n",
    "    \n",
    "    return group\n",
    "\n",
    "\n",
    "def process_video(group, sets_path):\n",
    "    # Get group ids and video\n",
    "    experiment_id = group[\"ExperimentId\"].values[0]\n",
    "    set_id = group[\"SetId\"].values[0]\n",
    "    sequence_id = group[\"SequenceId\"].values[0]\n",
    "    set_str = get_set_str(experiment_id=experiment_id, set_id=set_id)\n",
    "    video_path = f\"{sets_path}/experiment{experiment_id}/{set_str}/scene{sequence_id:02}.mp4\"\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Process frames\n",
    "    grouped_data = group.groupby(\"FrameId\")\n",
    "    results = grouped_data.progress_apply(process_frame, video=video)\n",
    "\n",
    "    video.release()\n",
    "\n",
    "    return pd.concat(results)\n",
    "\n",
    "\n",
    "def get_fixation_features(data: pd.DataFrame, sets_path: str) -> pd.DataFrame:\n",
    "    # Get depth features\n",
    "    bar = tqdm(total=2, desc=\"⌛ Loading depth features...\")\n",
    "    depth_maps = get_feature_data(DEPTH_MAP_PFM_PATH, \"pfm\")\n",
    "    bar.update(1)\n",
    "    depth_segs = get_feature_data(DEPTH_SEG_PFM_PATH, \"pfm\")\n",
    "    bar.update(1)\n",
    "    bar.close()\n",
    "\n",
    "    data[\"Depth\"] = data.progress_apply(\n",
    "        lambda x: get_feature_value(data=depth_maps, row=x),\n",
    "        axis=1,\n",
    "    )\n",
    "    data[\"DepthSeg\"] = data.progress_apply(\n",
    "        lambda x: get_feature_value(data=depth_segs, row=x),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Group by experiment, set, and sequence\n",
    "    grouped_data = data.groupby([\"ExperimentId\", \"SetId\", \"SequenceId\"])\n",
    "    results = grouped_data.apply(lambda group: process_video(group, sets_path))\n",
    "    results = results.reset_index(drop=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "fixation_data = get_fixation_features(fixation_data, SETS_PATH)\n",
    "null_fixation_data = get_fixation_features(null_fixation_data, SETS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fixation_data)\n",
    "display(null_fixation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_boxplots(fixation_data: pd.DataFrame, null_fixation_data: pd.DataFrame, features: list):\n",
    "    \"\"\"\n",
    "    Generate box plots for the specified features, comparing actual and null fixation data.\n",
    "\n",
    "    Parameters:\n",
    "        fixation_data (pd.DataFrame): DataFrame containing the actual fixation features.\n",
    "        null_fixation_data (pd.DataFrame): DataFrame containing the null fixation features.\n",
    "        features (list): List of feature names to plot.\n",
    "    \"\"\"\n",
    "    # Add a column to differentiate the actual and null data\n",
    "    fixation_data[\"Type\"] = \"Actual Fixations\"\n",
    "    null_fixation_data[\"Type\"] = \"Null Fixations\"\n",
    "    \n",
    "    # Combine both datasets\n",
    "    combined_data = pd.concat([fixation_data, null_fixation_data])\n",
    "\n",
    "    for feature in features:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=\"Type\", y=feature, data=combined_data)\n",
    "        plt.title(f'Box Plot of {feature} - Actual vs Null Fixations')\n",
    "        plt.ylabel(f'{feature}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "features_to_plot = [\n",
    "    \"Depth\", \"DepthSeg\", \"Brightness\", \"Colorfulness\", \"Saturation\",\n",
    "    \"Hue\", \"GradientMagnitude\", \"LocalContrast\", \"LocalSharpness\", \"LocalEntropy\"\n",
    "]\n",
    "\n",
    "# Plot box plots for each feature comparing actual and null fixation data\n",
    "plot_feature_boxplots(fixation_data, null_fixation_data, features_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
